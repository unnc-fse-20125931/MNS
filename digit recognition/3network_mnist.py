# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12CHrOyEQpSER35UqtFxVFVYll1ogxueG
"""

import torch
import torchvision
from torch import nn
from torch.utils.data import DataLoader
import torchvision.transforms as T
import matplotlib.pyplot as plt

BATCH_SIZE = 128
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("Current device", DEVICE.type)

trans = T.Compose([T.ToTensor(),T.Normalize((0.5,),(1.0,))])

train_set = torchvision.datasets.MNIST(root = './', train = True, transform = trans, download = True)
test_set = torchvision.datasets.MNIST(root = './', train = False, transform = trans)

train_loader = DataLoader(dataset=train_set,batch_size=BATCH_SIZE,shuffle = True)
test_loader = DataLoader(dataset=test_set,batch_size=BATCH_SIZE,shuffle = True)

print(train_set.__getitem__(0)[0].shape)

for i in range(16):
  plt.subplot(4,4,i + 1)
  plt.imshow(train_set.__getitem__(i)[0].squeeze().numpy(), cmap='Greys_r')
  plt.axis('off')
plt.show()

## Full connected
FullConnectedNet = nn.Sequential(
    nn.Linear(28 * 28 , 512),
    nn.ReLU(),
    nn.Linear(512, 256),
    nn.ReLU(),
    nn.Linear(256,10)
)

## CNN
class LeNet(nn.Module):
  def __init__(self):
    super(LeNet, self).__init__()
    self.conv1 = nn.Sequential(
        nn.Conv2d(1, 20, 5, 1),  ##  (28 + 2*0 -  5 ) / 1  + 1 =24
        nn.ReLU(),
        nn.MaxPool2d(2,2)  ##  (24+2*0 - 2) / 2 + 1 = 12
    )

    self.conv2 = nn.Sequential(
        nn.Conv2d(20, 50, 5, 1),  ##  (12 + 2*0 -  5 ) / 1  +  1 = 8
        nn.ReLU(),
        nn.MaxPool2d(2,2)  ##  ( 8 +2*0 - 2) / 2 + 1 = 4
    )
    self.linear = nn.Sequential(
        nn.Linear( 4 * 4 * 50, 500),
        nn.ReLU(),
        nn.Linear(500,10)
    )

  def forward(self,x):
    x = self.conv1(x)
    x = self.conv2(x)
    x = x.view(x.shape[0], -1)
    x = self.linear(x)
    return x

###  RNN
class GRUNet(nn.Module):
  def __init__(self):
    super(GRUNet, self).__init__()
    self.gru = nn.GRU(28, 100, batch_first = True)  ## 100: hidden layer
    self.linear = nn.Linear(100,10)

  def forward(self,x):
    x = x.squeeze()
    # x: [batch_size, 28, 28]  [batch, timestamp, input]
    out, _ = self.gru(x)
    ## out : [batch,28,100] [batch, timestamp, hidden]
    x = self.linear(out[:,-1,:])
    return x

#model = FullConnectedNet
model = LeNet()
#model = GRUNet()
model = model.to(DEVICE)

optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)
loss_function = nn.CrossEntropyLoss()

### train
for epoch in range(10):
  for index, (x,label) in enumerate(train_loader):
    x, label = x.to(DEVICE),label.to(DEVICE)
    #x = x.view(-1, 28 * 28) ## -1 equals the batch size ## uncommented only on FullConnectedNetwork
    out = model(x)
    #print(x)
    #print(label)
    #print(out)
    loss = loss_function(out,label)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (index+1) % 100 == 0 or (index+1) == len(train_loader):
      print("Train epoch", epoch, 'batch index', index+1, 'loss', float(loss))


  ### test
  count = correct = 0
  for index, (x,label) in enumerate(test_loader):
    x, label = x.to(DEVICE),label.to(DEVICE)
    #x = x.view(-1, 28*28) ## -1 equals the batch size ## uncommented only on FullConnectedNetwork
    out = model(x)  ## [batch_size, 10]
    loss = loss_function(out,label)
    _ , predict = torch.max(out,1)
    print(predict)
    count += x.shape[0]
    correct += (predict == label).sum()


    if (index+1) % 100 == 0 or (index+1) == len(test_loader):
      print("Train epoch", epoch, 'batch index', index+1, 'loss', float(loss), 'acc', correct*1.0/ count)